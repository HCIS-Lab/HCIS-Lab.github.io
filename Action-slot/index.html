<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Action-slot: Visual Action-centric Representations for Atomic Activity Recognition in Traffic Scenes"> 
  <meta name="keywords" content="Slot Attention, Atomic Activity Recognition, Action Recognition">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Action-slot: Visual Action-centric Representations for Atomic Activity Recognition in Traffic Scenes</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/logo.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>

  <script>
        let slideIndex = 1, slideIndex2 = 1, slideIndex3 = 1, slideIndex4 = 1;
        showSlides(slideIndex);
        showSlides2(slideIndex2);
        showSlides3(slideIndex3);
        showSlides4(slideIndex4);

        // Next/previous controls
        function plusSlides(n) {
          showSlides(slideIndex += n);
        }

        // Thumbnail image controls
        function currentSlide(n) {
          showSlides(slideIndex = n);
        }
        function currentSlide2(n) {
          showSlides2(slideIndex2 = n);
        }
        function currentSlide3(n) {
          showSlides3(slideIndex3 = n);
        }
        function currentSlide4(n) {
          showSlides4(slideIndex4 = n);
        }


        function showSlides(n) {
          let i;
          let slides = document.getElementsByClassName("mySlides");
          let dots = document.getElementsByClassName("dot");
          if (n > slides.length) {slideIndex = 1}
          if (n < 1) {slideIndex = slides.length}
          for (i = 0; i < slides.length; i++) {
            slides[i].style.display = "none";
          }
          for (i = 0; i < dots.length; i++) {
            dots[i].className = dots[i].className.replace(" active", "");
          }
          slides[slideIndex-1].style.display = "block";
          dots[slideIndex-1].className += " active";
        }

        function showSlides2(n) {
          let i;
          let slides = document.getElementsByClassName("mySlides2");
          let dots = document.getElementsByClassName("dot2");
          if (n > slides.length) {slideIndex2 = 1}
          if (n < 1) {slideIndex2 = slides.length}
          for (i = 0; i < slides.length; i++) {
            slides[i].style.display = "none";
          }
          for (i = 0; i < dots.length; i++) {
            dots[i].className = dots[i].className.replace(" active", "");
          }
          slides[slideIndex2-1].style.display = "block";
          dots[slideIndex2-1].className += " active";
        }

        function showSlides3(n) {
          let i;
          let slides = document.getElementsByClassName("mySlides3");
          let dots = document.getElementsByClassName("dot3");
          if (n > slides.length) {slideIndex3 = 1}
          if (n < 1) {slideIndex3 = slides.length}
          for (i = 0; i < slides.length; i++) {
            slides[i].style.display = "none";
          }
          for (i = 0; i < dots.length; i++) {
            dots[i].className = dots[i].className.replace(" active", "");
          }
          slides[slideIndex3-1].style.display = "block";
          dots[slideIndex3-1].className += " active";
        }
        
        function showSlides4(n) {
          let i;
          let slides = document.getElementsByClassName("mySlides4");
          let dots = document.getElementsByClassName("dot4");
          if (n > slides.length) {slideIndex4 = 1}
          if (n < 1) {slideIndex4 = slides.length}
          for (i = 0; i < slides.length; i++) {
            slides[i].style.display = "none";
          }
          for (i = 0; i < dots.length; i++) {
            dots[i].className = dots[i].className.replace(" active", "");
          }
          slides[slideIndex4-1].style.display = "block";
          dots[slideIndex4-1].className += " active";
        }
      </script>
      <script>
          window.dataLayer = window.dataLayer || [];

          function gtag() {
          dataLayer.push(arguments);
          }

          gtag('js', new Date());

          gtag('config', 'G-PYVRSFMDRL');
      </script>

      <script>
          window.dataLayer = window.dataLayer || [];

          function gtag() {
          dataLayer.push(arguments);
          }

          gtag('js', new Date());

          gtag('config', 'G-PYVRSFMDRL');
      </script>
</head>
<body>

<script type="module">
  showSlides(slideIndex);
  showSlides2(slideIndex2);
  showSlides3(slideIndex3);
  showSlides4(slideIndex4);
</script>
      


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Action-slot: Visual Action-centric Representations for Atomic Activity Recognition in Traffic Scenes</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block-name">
              <a href="https://hankkung.github.io/website/">Chi-Hsi Kung</a><sup>1</sup>,</span>
            <span class="author-block-name">
              <a href="">Shu-Wei Lu</a><sup>1</sup>,</span>
            <span class="author-block-name">
              <a href="https://sites.google.com/site/yihsuantsai/">Yi-Hsuan Tsai</a><sup>2</sup>,</span>
            <span class="author-block-name">
              <a href="https://sites.google.com/site/yitingchen0524">Yi-Ting Chen</a><sup>1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>National Yang Ming Chiao Tung University</span>
            <span class="author-block"><sup>2</sup>Google</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://drive.google.com/file/d/1rr0_wqIB_vnNr7O9Z9pxOrV69B7so-JI/view?usp=sharing"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <!-- <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
              <!-- Video Link. -->
              <!-- Dataset Link. -->
                <span class="link-block">
                    <a href="https://nycu1-my.sharepoint.com/:f:/g/personal/ychen_m365_nycu_edu_tw/EviA5ovlh6hPo_ZXEPQjxAQB2R3vNubk3HM1u4ib1VdPFA?e=WHEWdm"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="far fa-images"></i>
                    </span>
                    <span>Dataset</span>
                    </a>
                </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>

            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section is-max-desktop">
  <div class="container is-max-desktop" style="width:200%">
    <div class="column is-centered has-text-centered">
      <img src="./static/videos/action_slot_viz.gif">
    </div>
  </div>
</section>


<section class="section" id="abstract">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
          In this paper, we study multi-label atomic activity recognition. Despite the notable progress in action recognition, it is still challenging to recognize atomic activities due to a deficiency in holistic understanding of both multiple road usersâ€™ motions and their contextual information. In this paper, we introduce Action-slot, a slot attention-based approach that learns visual action-centric representations, capturing both motion and contextual information. Our key idea is to design action slots that are capable of paying attention to regions where atomic activities occur, without the need for explicit perception guidance. To further enhance slot attention, we introduce a background slot that competes with action slots, aiding the training process in avoiding unnecessary focus on background regions devoid of activities. Yet, the imbalanced class distribution in the existing dataset hampers the assessment of rare activities. To address the limitation, we collect a synthetic dataset called TACO, which is four times larger than OATS and features a balanced distribution of atomic activities. To validate the effectiveness of our method, we conduct comprehensive experiments and ablation studies against various action recognition baselines. We also show that the performance of multi-label atomic activity recognition on real-world datasets can be improved by pretraining representations on TACO. We will release our source code and dataset.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section is-max-desktop">
  <div class="container is-max-desktop">
    <div class="column is-centered has-text-centered">
      <h2 class="title is-3">Action-slot for Atomic Activity</h2>
    </div>
    <div class="content has-text-justified">
      <img src="./static/images/teaser.png">
    </div>
    <div class="content has-text-justified">
      <p>
        Illustration of the concept of multi-label atomic activity recognition and our proposed Action-slot. In the scene, three atomic activities are presented and depicted by colored arrows. For example, the red arrow represents the <strong>Z1-Z4: C+</strong> atomic activity, indicating a group of vehicles turning left. Atomic activities are defined based on road user's type and their motion patterns grounded in the underlying road structure. We introduce Action-slot to learn visual action-centric representations that enable decomposing multiple atomic activities in videos. We demonstrate that our framework can effectively recognize multiple atomic activities via learned representations.
      </p>
    </div>
  </div>
</section>


<hr>



<section class="section is-max-desktop">
  <div class="container is-max-desktop">
    <div class="column is-centered has-text-centered">
      <h2 class="title is-3">Visualization of Attention from Action-slot on Various Datasets</h2>
      <p class="has-text-left">
        We visualize attention from action slots. Distinct colored masks represent the classes of atomic activitiy that the corresponding action slots pay attention to.
      </p>
    </div>

    <div class="content has-text-justified has-text-left">
      <h3 style="margin-bottom: 5px;">The OATS Dataset [1]</h3>
      <div style="border-bottom: 1px solid #333; margin-bottom: 10px;"></div>

    </div>
    <div class="content has-text-justified">
      <img src="./static/images/oats_web.gif">
    </div>

    <div class="content has-text-justified has-text-left">
      <h3 style="margin-bottom: 5px;">The nuScenes Dataset [2]</h3>
      <div style="border-bottom: 1px solid #333; margin-bottom: 10px;"></div>
    </div>
    <div class="content has-text-justified">
      <img src="./static/images/nuscenes.gif" >
    </div>

    <div class="content has-text-justified has-text-left">
      <h3 style="margin-bottom: 5px;">Our TACO Dataset</h3>
      <div style="border-bottom: 1px solid #333; margin-bottom: 10px;"></div>
    </div>
    <div class="content has-text-justified">
      <img src="./static/images/taco_q.gif">
    </div>
  </div> 
</section>

<hr>

<section class="section is-max-desktop">
  <div class="container is-max-desktop">
    <div class="column is-centered has-text-centered">
      <h2 class="title is-3">Attention Visualization in Challenging Scenarios</h2>
    </div>

    <div class="content has-text-justified has-text-left">
      <h3 >Occluded Road Topology</h3>
      <p class="has-text-left" style="margin-bottom: 5px;">
        <b>(Left)</b> The intersection is partially occluded by the traffic cones. <b>(Right)</b> The Corner Z4 is entirely covered by the construction.
      </p>
    </div>
    <div class="content has-text-justified" style="margin-top: -10px;">
      <img src="./static/images/construction.gif">
    </div>

    <div class="content has-text-justified has-text-left">
      <h3>Road User with Multiple Actions</h3>
      <p class="has-text-left" style="margin-bottom: 5px;">
        The pedestrian perform two actions consecutively. Action-slot can recognize the transition of actions.
      </p>
    </div>
    <div class="content has-text-justified" style="margin-top: -10px;">
      <img src="./static/images/action_transition.gif" >
    </div>

  </div> 
</section>

<hr>

<section class="section is-max-desktop">
  <div class="container is-max-desktop">
    <div class="column is-centered has-text-centered">
      <h2 class="title is-3">Object Guidance vs. The Proposed Guidance</h2>
      <p class="has-text-left">
        We compare object instance mask guidance with Action-slot that is guided with L_{bg} and L_{neg} mentioned in Sec. 3.3. . We hpothesize that the object guidance can mislead the model because not all road users are involved in an activity. To verify it, we first create scenarios where presents many static pedestrians. Then we visualize the attention from the any slot that predicts false positive. Note that there is no atomic activity presented in the scenarios. The object-guided model is confused by the static road users and makes false positive predictions. While our Action-slot show strong robustness in the scenarios.
      </p>
    </div>
    <div class="content has-text-justified">
      <img src="./static/images/fp.gif">
    </div>

  </div> 
</section>


<hr>


<div class="container is-max-desktop content">
  <div class="columns is-centered">
    <div class="column is-four-fifths">
      <p>
        [1] Agarwal et al., "Ordered Atomic Activity for Fine-grained Interactive Traffic Scenario Understanding". ICCV 2023
      </p>
      <p>
        [2] Caesar et al., "nuscenes: A multimodal dataset for autonomous driving". CVPR 2020
      </p>
    </div>
  </div>
</div>



<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{action_slot2023,
  title={Action-slot: Visual Action-centric Representations for Atomic Activity Recognition in Traffic Scenes},
  author={Chi-Hsi Kung, Shu-Wei Lu, Yi-Hsuan Tsai, Yi-Ting Chen},
  year={2023},
  booktitle={arXiv}
}
    </code></pre>
  </div>
</section>





<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/images/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            Website template modified from <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">NeRFies</a>.
          <p>
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
