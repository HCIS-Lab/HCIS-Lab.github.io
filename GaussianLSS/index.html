<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Academic Project Page</title>
  <link rel="icon" type="image/x-icon" href="static/images/hcislab.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <script type="text/javascript" async
  src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script type="text/javascript" async
    id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-0.1 publication-title">GaussianLSS: Toward Real-world BEV Perception: Depth Uncertainty Estimation
              via Gaussian Splatting</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">Shu-Wei Lu</a><sup>1</sup>,</span>
                <span class="author-block">
                  <a href="https://sites.google.com/site/yihsuantsai/" target="_blank">Yi-Hsuan Tsai</a><sup>2</sup>,</span>
                  <span class="author-block">
                    <a href="https://sites.google.com/site/yitingchen0524/home" target="_blank">Yi-Ting Chen</a><sup>1*</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">National Yang Ming Chiao Tung University<sup>1</sup> Atmanity Inc.<sup>2</sup><br>CVPR 2025</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>: Corresponding author</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/HCIS-Lab/GaussianLSS" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- <video poster="" id="tree" autoplay controls muted loop height="100%"> -->
        <!-- Your video here -->
        <!-- <source src="static/videos/banner_video.mp4" -->
        <!-- type="video/mp4"> -->
      <!-- </video> -->
      <h2 class="subtitle has-text-centered">
        <!-- Subtitle text  -->
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified" style="font-size: 14px;">
          <p>
              Birdâ€™s-eye view (BEV) perception has gained significant attention because it provides a unified representation to fuse
              multiple view images and enables a wide range of down-stream autonomous driving tasks, such as forecasting and
              planning. Recent state-of-the-art models utilize projection-based methods which formulate BEV perception as query
              learning to bypass explicit depth estimation. While we observe promising advancements in this paradigm, they still
              fall short of real-world applications because of the lack of
              uncertainty modeling and expensive computational requirement. In this work, we introduce GaussianLSS, a novel
              uncertainty-aware BEV perception framework that revisits unprojection-based methods, specifically the Lift-Splat-Shoot (LSS) paradigm, and enhances them with depth un-certainty modeling. GaussianLSS represents spatial dispersion by learning a soft depth mean and computing the variance of the depth distribution, which implicitly cap-
              tures object extents. We then transform the depth distribution into 3D Gaussians and rasterize them to construct
              uncertainty-aware BEV features. We evaluate GaussianLSS
              on the nuScenes dataset, achieving state-of-the-art performance compared to unprojection-based methods. In particular, it provides significant advantages in speed, running
              2x faster, and in memory efficiency, using 0.3x less memory compared to projection-based methods, while achieving competitive performance with only a 0.7% IoU difference.
          </p>      
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->
 
<section class="section">
  <div class="container is-max-desktop">
    <!-- Methodology. -->
    <section class="section is-small">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">GaussianLSS</h2>
          </div>
        </div>


        <br />
        <!-- Lifting -->
        <div class="content is-centered">
          <h2 class="title is-4" style="text-align: center;">Lifting</h2>
          <div class="columns is-centered has-text-centered">
            <figure style="display: flex; flex-direction: column; align-items: center;">
              <img src="./static/images/Splatting.png" alt="splatting"
                style="width: 110%; max-width: 140%; height: auto;" />
              <figcaption>
                <!-- <p>
                  Comparison between the lifting method of Lift-Splat-Shoot and GaussianLSS. 
                  (a) Lift-Splat-Shoot paradigm faces two key challenges in depth estimation. 
                  Sparse BEV projection arises from discretized depth bins, leading to incomplete spatial coverage and reduced perception accuracy. 
                  Additionally, unstable depth distribution occurs due to softmax-based probability assignment, where small depth variations cause inconsistent BEV features, disrupting spatial coherence. 
                  (b) addresses these issues by introducing uncertainty-aware depth modeling, which provides a continuous depth representation to reduce sparsity in BEV projection. Instead of relying on discrete bins, our method models depth as a distribution with uncertainty, ensuring smoother and more consistent feature aggregation. This approach mitigates the instability caused by softmax, allowing similar depths to receive proportionate attention and improving spatial coherence in BEV representations.
                </p> -->
              </figcaption>
            </figure>
          </div>
          <div class="content has-text-justified" style="font-size: 14px;">
            <p>
              (a) Lift-Splat-Shoot paradigm faces two key challenges in depth estimation. 
              Sparse BEV projection arises from discretized depth bins, leading to incomplete spatial coverage and reduced perception accuracy. 
              Additionally, unstable depth distribution occurs due to softmax-based probability assignment, where small depth variations cause inconsistent BEV features, disrupting spatial coherence. 
              (b) GaussianLSS introduces uncertainty-aware depth modeling, using a continuous depth representation to reduce sparsity and improve feature consistency. 
              Instead of relying on discrete bins, we compute the depth mean \( \mu \) and uncertainty \( \sigma \) from the predicted depth distribution, replacing softmax weighting with an uncertainty-aware range \([\mu - k\sigma, \mu + k\sigma]\). 
              The parameter \(k\) serves as an error tolerance coefficient, controlling the spread around the mean depth, ensuring smoother BEV projections and better spatial coherence.
            </p>      
          </div>
        </div>
        <!-- Structure -->
        <div class="content is-centered">
          <h2 class="title is-4" style="text-align: center;">Method Overview</h2>
          <div class="columns is-centered has-text-centered">
            <figure style="display: flex; flex-direction: column; align-items: center;">
              <img src="./static/images/Overview.png" alt="model"
                style="width: 110%; max-width: 140%; height: auto;" />
            </figure>
          </div>
          <div class="content has-text-justified" style="font-size: 14px;">
            <p>
              Multi-view images are first processed by a backbone network to extract features. 
              They are then input to a simple CNN layer to obtain splat features \( F_i \), opacity \( \alpha_i \), and depth distribution \( P_i \). 
              The predicted depth distribution undergoes an uncertainty transformation to produce a 3D uncertainty \( x_i \). 
              Next, BEV features are obtained through a splatting process, integrating feature distributions across views. 
              The resulting BEV features \( \mathbf{F}_{\text{BEV}} \), enriched with uncertainty awareness, are used as input to the task-specific head for prediction.
            </p>      
          </div>
        </div>
        <!-- Experiment -->
        <div class="content is-centered">
          <h2 class="title is-4" style="text-align: center;">Experiment</h2>
          <div class="columns is-centered has-text-centered">
            <figure style="display: flex; flex-direction: column; align-items: center;">
              <img src="./static/images/Experiment.png" alt="experiment"
                style="width: 80%; max-width: 90%; height: auto;" />
              <figcaption>
                <!-- <p>
                  Comparison between the lifting method of Lift-Splat-Shoot and GaussianLSS. 
                  (a) Lift-Splat-Shoot paradigm faces two key challenges in depth estimation. 
                  Sparse BEV projection arises from discretized depth bins, leading to incomplete spatial coverage and reduced perception accuracy. 
                  Additionally, unstable depth distribution occurs due to softmax-based probability assignment, where small depth variations cause inconsistent BEV features, disrupting spatial coherence. 
                  (b) addresses these issues by introducing uncertainty-aware depth modeling, which provides a continuous depth representation to reduce sparsity in BEV projection. Instead of relying on discrete bins, our method models depth as a distribution with uncertainty, ensuring smoother and more consistent feature aggregation. This approach mitigates the instability caused by softmax, allowing similar depths to receive proportionate attention and improving spatial coherence in BEV representations.
                </p> -->
              </figcaption>
            </figure>
          </div>
          <div class="content has-text-justified" style="font-size: 14px;">
            <p>
              GaussianLSS achieves state-of-the-art performance among 2D unprojection baselines. 
              In addition, it also demonstrates competitive performance compared to 3D projection-based methods, while offering significant advantages in memory efficiency and inference speed.
            </p>      
          </div>
        </div>
      </div>
    <section>
  </div>
</section>
<!-- Image below the abstract -->

<!-- Paper poster -->
 
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->

<!-- Experiments -->

<!-- End Experiments -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        @inproceedings{lu2025GaussianLSS,
        author    = {Shu-Wei Lu and Yi-Hsuan Tsai and Yi-Ting Chen},
        title     = {Toward Real-world BEV Perception: Depth Uncertainty Estimation via Gaussian Splatting},
        booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
        year      = {2025}
      }
    </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from theÂ <a href="https://nerfies.github.io" target="_blank">Nerfies</a>Â project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
